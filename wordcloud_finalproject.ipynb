{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzQB2bI1NvI/bWygEChCe9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShieldEdu/G4-finalproject/blob/main/wordcloud_finalproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade googletrans==3.1.0a0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "id": "1GqBhDh3uMSj",
        "outputId": "fbbc2a01-048d-4233-d28c-56f98c64a866"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n",
            "  Using cached httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.6.2)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Using cached httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n",
            "  Using cached h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16353 sha256=0e7309438c3bf8bb7a9f65962cdb20573e62f30d00bc4324a1df8966d16a0106\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/5d/3c/8477d0af4ca2b8b1308812c09f1930863caeebc762fe265a95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: h11, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.5\n",
            "    Uninstalling httpcore-1.0.5:\n",
            "      Successfully uninstalled httpcore-1.0.5\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.27.0\n",
            "    Uninstalling httpx-0.27.0:\n",
            "      Successfully uninstalled httpx-0.27.0\n",
            "  Attempting uninstall: googletrans\n",
            "    Found existing installation: googletrans 3.0.0\n",
            "    Uninstalling googletrans-3.0.0:\n",
            "      Successfully uninstalled googletrans-3.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastapi 0.111.0 requires httpx>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio 4.33.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio-client 0.17.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed googletrans-3.1.0a0 h11-0.9.0 httpcore-0.9.1 httpx-0.13.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h11",
                  "httpcore",
                  "httpx"
                ]
              },
              "id": "4355469855d84f59a4e9554e4d47e6ed"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9JcxNq5yt3-v",
        "outputId": "1d705e16-a6bc-4045-dad0-88df425b08f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-4.33.0-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.17.0 (from gradio)\n",
            "  Downloading gradio_client-0.17.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.4.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio)\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.1)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.17.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.17.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio)\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=17d8baa37d12b3e1a663f34b573f8589964764fe72b8c8fb8ada563f3905f095\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, uvloop, ujson, tomlkit, shellingham, semantic-version, ruff, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, aiofiles, watchfiles, uvicorn, starlette, httpcore, email_validator, typer, httpx, gradio-client, fastapi-cli, fastapi, gradio\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.9.0\n",
            "    Uninstalling h11-0.9.0:\n",
            "      Successfully uninstalled h11-0.9.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 0.9.1\n",
            "    Uninstalling httpcore-0.9.1:\n",
            "      Successfully uninstalled httpcore-0.9.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.13.3\n",
            "    Uninstalling httpx-0.13.3:\n",
            "      Successfully uninstalled httpx-0.13.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "googletrans 3.0.0 requires httpx==0.13.3, but you have httpx 0.27.0 which is incompatible.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 dnspython-2.6.1 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 gradio-4.33.0 gradio-client-0.17.0 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 orjson-3.10.3 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 ruff-0.4.8 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.37.2 tomlkit-0.12.0 typer-0.12.3 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h11"
                ]
              },
              "id": "71a7386ad8bf480ab2d483b8312dae22"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJuKjhW2tl3Y",
        "outputId": "2b16c552-f098-4781-cd10-2bba78c5d0a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.6.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans)\n",
            "  Downloading hstspreload-2024.6.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15718 sha256=2158ebe21be206606c720f015d51199ad3bc8426a337415557e0c0b2c27add71\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.6.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import stopwords\n",
        "import gradio as gr\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "translator = Translator()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def process_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    word_freq = Counter(words)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    return word_freq, pos_tags\n",
        "\n",
        "def generate_wordcloud(word_freq):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.savefig('wordcloud.png')\n",
        "    return 'wordcloud.png'\n",
        "\n",
        "def translate_and_get_pos(word_freq, pos_tags):\n",
        "    word_data = []\n",
        "    for word, freq in word_freq.items():\n",
        "        translation = translator.translate(word, dest='en').text\n",
        "        pos = [pos_tag[1] for pos_tag in pos_tags if pos_tag[0] == word]\n",
        "        pos = pos[0] if pos else 'N/A'\n",
        "        word_data.append((word, freq, translation, pos))\n",
        "    word_data.sort(key=lambda x: x[1], reverse=True)\n",
        "    return word_data\n",
        "\n",
        "def main(text):\n",
        "    word_freq, pos_tags = process_text(text)\n",
        "    wordcloud_image = generate_wordcloud(word_freq)\n",
        "    word_data = translate_and_get_pos(word_freq, pos_tags)\n",
        "\n",
        "    word_data_str = \"\\n\".join([f\"Word: {word}, Frequency: {freq}, Translation: {translation}, Part of Speech: {pos}\" for word, freq, translation, pos in word_data])\n",
        "    return wordcloud_image, word_data_str\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=main,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"image\", \"text\"],\n",
        "    title=\"Language Learning App\",\n",
        "    description=\"Input text to generate a word cloud and a frequency list with English meanings and parts of speech.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "epkTEH24tY0b",
        "outputId": "a8acdda7-ac93-4977-f7b4-b11ff89b5b61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ba87e1c027de6d234e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ba87e1c027de6d234e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import stopwords\n",
        "import gradio as gr\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "translator = Translator()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def process_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    word_freq = Counter(words)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    return word_freq, pos_tags\n",
        "\n",
        "def generate_wordcloud(word_freq):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.savefig('wordcloud.png')\n",
        "    return 'wordcloud.png'\n",
        "\n",
        "def translate_and_get_pos(word_freq, pos_tags):\n",
        "    pos_map = {\n",
        "        'NN': 'n.', 'NNS': 'n.', 'NNP': 'n.', 'NNPS': 'n.',\n",
        "        'VB': 'v.', 'VBD': 'v.', 'VBG': 'v.', 'VBN': 'v.', 'VBP': 'v.', 'VBZ': 'v.',\n",
        "        'JJ': 'adj.', 'JJR': 'adj.', 'JJS': 'adj.',\n",
        "        'RB': 'adv.', 'RBR': 'adv.', 'RBS': 'adv.'\n",
        "    }\n",
        "\n",
        "    word_data = []\n",
        "    for word, freq in word_freq.items():\n",
        "        translation = translator.translate(word, dest='ko').text\n",
        "        pos = [pos_tag[1] for pos_tag in pos_tags if pos_tag[0] == word]\n",
        "        pos = pos_map.get(pos[0], 'N/A') if pos else 'N/A'\n",
        "        word_data.append((word, freq, translation, pos))\n",
        "    word_data.sort(key=lambda x: x[1], reverse=True)\n",
        "    return word_data\n",
        "\n",
        "def main(text):\n",
        "    word_freq, pos_tags = process_text(text)\n",
        "    wordcloud_image = generate_wordcloud(word_freq)\n",
        "    word_data = translate_and_get_pos(word_freq, pos_tags)\n",
        "\n",
        "    word_data_str = \"\\n\".join([f\"{i+1}. {word}: {pos} {translation}\" for i, (word, freq, translation, pos) in enumerate(word_data)])\n",
        "    return wordcloud_image, word_data_str\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=main,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"image\", \"text\"],\n",
        "    title=\"Language Learning App\",\n",
        "    description=\"Input text to generate a word cloud and a frequency list with Korean meanings and parts of speech.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "UttKrzX3vh5W",
        "outputId": "163fcc42-d76e-4842-b39f-f75d103e7d5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ade940f3494463a3e1.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ade940f3494463a3e1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import stopwords\n",
        "import gradio as gr\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "translator = Translator()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def process_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    word_freq = Counter(words)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    return word_freq, pos_tags\n",
        "\n",
        "def generate_wordcloud(word_freq):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.savefig('wordcloud.png')\n",
        "    return 'wordcloud.png'\n",
        "\n",
        "def translate_and_get_pos(word_freq, pos_tags):\n",
        "    pos_map = {\n",
        "        'NN': 'n.', 'NNS': 'n.', 'NNP': 'n.', 'NNPS': 'n.',\n",
        "        'VB': 'v.', 'VBD': 'v.', 'VBG': 'v.', 'VBN': 'v.', 'VBP': 'v.', 'VBZ': 'v.',\n",
        "        'JJ': 'adj.', 'JJR': 'adj.', 'JJS': 'adj.',\n",
        "        'RB': 'adv.', 'RBR': 'adv.', 'RBS': 'adv.'\n",
        "    }\n",
        "\n",
        "    word_data = []\n",
        "    for word, freq in word_freq.items():\n",
        "        translation = translator.translate(word, dest='ko').text\n",
        "        pos = [pos_tag[1] for pos_tag in pos_tags if pos_tag[0] == word]\n",
        "        pos = pos_map.get(pos[0], 'N/A') if pos else 'N/A'\n",
        "        example_sentence = f\"ex) The word '{word}' in a sentence.\"\n",
        "        word_data.append((word, freq, translation, pos, example_sentence))\n",
        "    word_data.sort(key=lambda x: x[1], reverse=True)\n",
        "    return word_data[:50]\n",
        "\n",
        "def main(text):\n",
        "    word_freq, pos_tags = process_text(text)\n",
        "    wordcloud_image = generate_wordcloud(word_freq)\n",
        "    word_data = translate_and_get_pos(word_freq, pos_tags)\n",
        "\n",
        "    word_data_str = \"\\n\".join([f\"{i+1}. {word}: {pos} {translation}, {example_sentence}\" for i, (word, freq, translation, pos, example_sentence) in enumerate(word_data)])\n",
        "    return wordcloud_image, word_data_str\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=main,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"image\", \"text\"],\n",
        "    title=\"Language Learning App\",\n",
        "    description=\"Input text to generate a word cloud and a frequency list with Korean meanings and parts of speech.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "Y8D96WjIuLUf",
        "outputId": "68f7564f-21d1-41f7-c9d9-946c0974108c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://4b3e5d1e97f39f870a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4b3e5d1e97f39f870a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import stopwords\n",
        "import gradio as gr\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "translator = Translator()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "example_sentences = {\n",
        "    \"village\": \"The small village was surrounded by green fields.\",\n",
        "    \"adventure\": \"They went on an exciting adventure in the forest.\",\n",
        "    \"alex\": \"Alex was the bravest kid in our class.\",\n",
        "    \"map\": \"We used a map to find the hidden treasure.\",\n",
        "    \"guardian\": \"The guardian watched over the ancient ruins.\",\n",
        "    \"echo\": \"Her voice echoed through the empty hall.\",\n",
        "    \"ridge\": \"They climbed up to the ridge for a better view.\",\n",
        "    \"cave\": \"They explored a dark cave in the mountains.\",\n",
        "    \"among\": \"She found her book among the pile of papers.\",\n",
        "    \"glen\": \"The glen was filled with wildflowers and a bubbling stream.\",\n",
        "    \"mountains\": \"The mountains were covered with snow in winter.\",\n",
        "    \"children\": \"The children played games in the park.\",\n",
        "    \"known\": \"He was known for his kindness and bravery.\",\n",
        "    \"hidden\": \"They found a hidden door behind the bookshelf.\",\n",
        "    \"local\": \"The local market was full of fresh produce.\",\n",
        "    \"discovery\": \"The discovery of the old map excited everyone.\",\n",
        "    \"eagle\": \"An eagle soared high above the valley.\",\n",
        "    \"villagers\": \"The villagers gathered in the square for the festival.\",\n",
        "    \"legend\": \"The legend of the lost city intrigued the adventurers.\",\n",
        "    \"tales\": \"Grandma told us tales of her childhood.\",\n",
        "    \"daring\": \"His daring escape from the cave was legendary.\",\n",
        "    \"spirit\": \"The spirit of adventure was alive in their hearts.\",\n",
        "    \"exploring\": \"They spent the summer exploring the forest.\",\n",
        "    \"old\": \"The old castle was full of secrets.\",\n",
        "    \"lost\": \"He felt lost without his best friend.\",\n",
        "    \"ancient\": \"They discovered ancient artifacts in the desert.\",\n",
        "    \"inside\": \"Inside the box was a beautiful necklace.\",\n",
        "    \"treasure\": \"They dreamed of finding hidden treasure.\",\n",
        "    \"whispering\": \"The trees were whispering secrets in the wind.\",\n",
        "    \"hollow\": \"They found a hollow tree to hide in during the storm.\",\n",
        "    \"decided\": \"She decided to take the long way home.\",\n",
        "    \"journey\": \"Their journey took them across the country.\",\n",
        "    \"mia\": \"Mia always carried a notebook with her.\",\n",
        "    \"sam\": \"Sam enjoyed building model airplanes.\",\n",
        "    \"together\": \"They worked together to solve the mystery.\",\n",
        "    \"way\": \"She found a new way to solve the puzzle.\",\n",
        "    \"reached\": \"They finally reached the top of the hill.\",\n",
        "    \"chest\": \"The chest was filled with gold coins.\",\n",
        "    \"boulder\": \"A large boulder blocked the path.\",\n",
        "    \"artifacts\": \"The museum displayed artifacts from ancient Egypt.\",\n",
        "    \"legends\": \"The legends spoke of a hidden kingdom.\",\n",
        "    \"explore\": \"They wanted to explore the old mansion.\",\n",
        "    \"secret\": \"She kept the secret hidden from everyone.\",\n",
        "    \"small\": \"The small kitten was very playful.\",\n",
        "    \"mountain\": \"The mountain was covered in thick forests.\",\n",
        "    \"part\": \"Each part of the puzzle was important.\",\n",
        "    \"everyday\": \"He wore his everyday clothes to the party.\",\n",
        "    \"life\": \"Life in the village was peaceful.\",\n",
        "    \"nestled\": \"The cabin was nestled in the woods.\",\n",
        "    \"towering\": \"The towering trees made the forest dark and cool.\"\n",
        "}\n",
        "\n",
        "def process_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    word_freq = Counter(words)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    return word_freq, pos_tags\n",
        "\n",
        "def generate_wordcloud(word_freq):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.savefig('wordcloud.png')\n",
        "    return 'wordcloud.png'\n",
        "\n",
        "def translate_and_get_pos(word_freq, pos_tags):\n",
        "    pos_map = {\n",
        "        'NN': 'n.', 'NNS': 'n.', 'NNP': 'n.', 'NNPS': 'n.',\n",
        "        'VB': 'v.', 'VBD': 'v.', 'VBG': 'v.', 'VBN': 'v.', 'VBP': 'v.', 'VBZ': 'v.',\n",
        "        'JJ': 'adj.', 'JJR': 'adj.', 'JJS': 'adj.',\n",
        "        'RB': 'adv.', 'RBR': 'adv.', 'RBS': 'adv.'\n",
        "    }\n",
        "\n",
        "    word_data = []\n",
        "    for word, freq in word_freq.items():\n",
        "        translation = translator.translate(word, dest='ko').text\n",
        "        pos = [pos_tag[1] for pos_tag in pos_tags if pos_tag[0] == word]\n",
        "        pos = pos_map.get(pos[0], 'N/A') if pos else 'N/A'\n",
        "        example_sentence = example_sentences.get(word, f\"ex) The word '{word}' in a sentence.\")\n",
        "        word_data.append((word, freq, translation, pos, example_sentence))\n",
        "    word_data.sort(key=lambda x: x[1], reverse=True)\n",
        "    return word_data[:50]\n",
        "\n",
        "def main(text):\n",
        "    word_freq, pos_tags = process_text(text)\n",
        "    wordcloud_image = generate_wordcloud(word_freq)\n",
        "    word_data = translate_and_get_pos(word_freq, pos_tags)\n",
        "\n",
        "    word_data_str = \"\\n\".join([f\"{i+1}. {word}: {pos} {translation}, {example_sentence}\" for i, (word, freq, translation, pos, example_sentence) in enumerate(word_data)])\n",
        "    return wordcloud_image, word_data_str\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=main,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"image\", \"text\"],\n",
        "    title=\"Language Learning App\",\n",
        "    description=\"Input text to generate a word cloud and a frequency list with Korean meanings and parts of speech.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "0ATXubNBtj5I",
        "outputId": "d8f62084-f8e0-4efb-d7d9-dceb96002f58"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://4feea6d9968d07eca5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4feea6d9968d07eca5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import stopwords\n",
        "import gradio as gr\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "translator = Translator()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "example_sentences = {\n",
        "    \"village\": \"The small village was surrounded by green fields.\",\n",
        "    \"adventure\": \"They went on an exciting adventure in the forest.\",\n",
        "    \"map\": \"We used a map to find the hidden treasure.\",\n",
        "    \"guardian\": \"The guardian watched over the ancient ruins.\",\n",
        "    \"echo\": \"Her voice echoed through the empty hall.\",\n",
        "    \"ridge\": \"They climbed up to the ridge for a better view.\",\n",
        "    \"cave\": \"They explored a dark cave in the mountains.\",\n",
        "    \"among\": \"She found her book among the pile of papers.\",\n",
        "    \"glen\": \"The glen was filled with wildflowers and a bubbling stream.\",\n",
        "    \"mountains\": \"The mountains were covered with snow in winter.\",\n",
        "    \"children\": \"The children played games in the park.\",\n",
        "    \"known\": \"He was known for his kindness and bravery.\",\n",
        "    \"hidden\": \"They found a hidden door behind the bookshelf.\",\n",
        "    \"local\": \"The local market was full of fresh produce.\",\n",
        "    \"discovery\": \"The discovery of the old map excited everyone.\",\n",
        "    \"eagle\": \"An eagle soared high above the valley.\",\n",
        "    \"villagers\": \"The villagers gathered in the square for the festival.\",\n",
        "    \"legend\": \"The legend of the lost city intrigued the adventurers.\",\n",
        "    \"tales\": \"Grandma told us tales of her childhood.\",\n",
        "    \"daring\": \"His daring escape from the cave was legendary.\",\n",
        "    \"spirit\": \"The spirit of adventure was alive in their hearts.\",\n",
        "    \"exploring\": \"They spent the summer exploring the forest.\",\n",
        "    \"old\": \"The old castle was full of secrets.\",\n",
        "    \"lost\": \"He felt lost without his best friend.\",\n",
        "    \"ancient\": \"They discovered ancient artifacts in the desert.\",\n",
        "    \"inside\": \"Inside the box was a beautiful necklace.\",\n",
        "    \"treasure\": \"They dreamed of finding hidden treasure.\",\n",
        "    \"whispering\": \"The trees were whispering secrets in the wind.\",\n",
        "    \"hollow\": \"They found a hollow tree to hide in during the storm.\",\n",
        "    \"decided\": \"She decided to take the long way home.\",\n",
        "    \"journey\": \"Their journey took them across the country.\",\n",
        "    \"together\": \"They worked together to solve the mystery.\",\n",
        "    \"way\": \"She found a new way to solve the puzzle.\",\n",
        "    \"reached\": \"They finally reached the top of the hill.\",\n",
        "    \"chest\": \"The chest was filled with gold coins.\",\n",
        "    \"boulder\": \"A large boulder blocked the path.\",\n",
        "    \"artifacts\": \"The museum displayed artifacts from ancient Egypt.\",\n",
        "    \"legends\": \"The legends spoke of a hidden kingdom.\",\n",
        "    \"explore\": \"They wanted to explore the old mansion.\",\n",
        "    \"secret\": \"She kept the secret hidden from everyone.\",\n",
        "    \"small\": \"The small kitten was very playful.\",\n",
        "    \"mountain\": \"The mountain was covered in thick forests.\",\n",
        "    \"part\": \"Each part of the puzzle was important.\",\n",
        "    \"everyday\": \"He wore his everyday clothes to the party.\",\n",
        "    \"life\": \"Life in the village was peaceful.\",\n",
        "    \"nestled\": \"The cabin was nestled in the woods.\",\n",
        "    \"towering\": \"The towering trees made the forest dark and cool.\"\n",
        "}\n",
        "\n",
        "def process_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    word_freq = Counter(words)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    return word_freq, pos_tags\n",
        "\n",
        "def generate_wordcloud(word_freq):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.savefig('wordcloud.png')\n",
        "    return 'wordcloud.png'\n",
        "\n",
        "def translate_and_get_pos(word_freq, pos_tags):\n",
        "    pos_map = {\n",
        "        'NN': 'n.', 'NNS': 'n.', 'NNP': 'n.', 'NNPS': 'n.',\n",
        "        'VB': 'v.', 'VBD': 'v.', 'VBG': 'v.', 'VBN': 'v.', 'VBP': 'v.', 'VBZ': 'v.',\n",
        "        'JJ': 'adj.', 'JJR': 'adj.', 'JJS': 'adj.',\n",
        "        'RB': 'adv.', 'RBR': 'adv.', 'RBS': 'adv.'\n",
        "    }\n",
        "\n",
        "    word_data = []\n",
        "    for word, freq in word_freq.items():\n",
        "        pos = [pos_tag[1] for pos_tag in pos_tags if pos_tag[0] == word]\n",
        "        if pos and pos[0] in ['NNP', 'NNPS']:\n",
        "            continue  # Skip proper nouns\n",
        "        translation = translator.translate(word, dest='ko').text\n",
        "        pos = pos_map.get(pos[0], 'N/A') if pos else 'N/A'\n",
        "        example_sentence = example_sentences.get(word, f\"ex) The word '{word}' in a sentence.\")\n",
        "        word_data.append((word, freq, translation, pos, example_sentence))\n",
        "    word_data.sort(key=lambda x: x[1], reverse=True)\n",
        "    return word_data[:50]\n",
        "\n",
        "def main(text):\n",
        "    word_freq, pos_tags = process_text(text)\n",
        "    wordcloud_image = generate_wordcloud(word_freq)\n",
        "    word_data = translate_and_get_pos(word_freq, pos_tags)\n",
        "\n",
        "    word_data_str = \"\\n\".join([f\"{i+1}. {word}: {pos} {translation}, {example_sentence}\" for i, (word, freq, translation, pos, example_sentence) in enumerate(word_data)])\n",
        "    return wordcloud_image, word_data_str\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=main,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"image\", \"text\"],\n",
        "    title=\"Language Learning App\",\n",
        "    description=\"Input text to generate a word cloud and a frequency list with Korean meanings and parts of speech.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "zaHjTxt0ynFB",
        "outputId": "33599b67-5576-4377-f617-30323c2754e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://6822e7e7d0319dc777.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6822e7e7d0319dc777.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import stopwords\n",
        "import gradio as gr\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "translator = Translator()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "example_sentences = {\n",
        "    \"villager\": \"Villagers are good people.\",\n",
        "    \"adventure\": \"They went on an exciting adventure in the forest.\",\n",
        "    \"map\": \"We used a map to find the hidden treasure.\",\n",
        "    \"guardian\": \"The guardian watched over the ancient ruins.\",\n",
        "    \"echo\": \"Her voice echoed through the empty hall.\",\n",
        "    \"ridge\": \"They climbed up to the ridge for a better view.\",\n",
        "    \"cave\": \"They explored a dark cave in the mountains.\",\n",
        "    \"among\": \"She found her book among the pile of papers.\",\n",
        "    \"glen\": \"The glen was filled with wildflowers and a bubbling stream.\",\n",
        "    \"mountains\": \"The mountains were covered with snow in winter.\",\n",
        "    \"children\": \"The children played games in the park.\",\n",
        "    \"known\": \"He was known for his kindness and bravery.\",\n",
        "    \"hidden\": \"They found a hidden door behind the bookshelf.\",\n",
        "    \"local\": \"The local market was full of fresh produce.\",\n",
        "    \"discovery\": \"The discovery of the old map excited everyone.\",\n",
        "    \"eagle\": \"An eagle soared high above the valley.\",\n",
        "    \"legend\": \"The legend of the lost city intrigued the adventurers.\",\n",
        "    \"tales\": \"Grandma told us tales of her childhood.\",\n",
        "    \"daring\": \"His daring escape from the cave was legendary.\",\n",
        "    \"spirit\": \"The spirit of adventure was alive in their hearts.\",\n",
        "    \"exploring\": \"They spent the summer exploring the forest.\",\n",
        "    \"old\": \"The old castle was full of secrets.\",\n",
        "    \"lost\": \"He felt lost without his best friend.\",\n",
        "    \"ancient\": \"They discovered ancient artifacts in the desert.\",\n",
        "    \"inside\": \"Inside the box was a beautiful necklace.\",\n",
        "    \"treasure\": \"They dreamed of finding hidden treasure.\",\n",
        "    \"whispering\": \"The trees were whispering secrets in the wind.\",\n",
        "    \"hollow\": \"They found a hollow tree to hide in during the storm.\",\n",
        "    \"decided\": \"She decided to take the long way home.\",\n",
        "    \"journey\": \"Their journey took them across the country.\",\n",
        "    \"together\": \"They worked together to solve the mystery.\",\n",
        "    \"way\": \"She found a new way to solve the puzzle.\",\n",
        "    \"reached\": \"They finally reached the top of the hill.\",\n",
        "    \"chest\": \"The chest was filled with gold coins.\",\n",
        "    \"boulder\": \"A large boulder blocked the path.\",\n",
        "    \"artifacts\": \"The museum displayed artifacts from ancient Egypt.\",\n",
        "    \"legends\": \"The legends spoke of a hidden kingdom.\",\n",
        "    \"explore\": \"They wanted to explore the old mansion.\",\n",
        "    \"secret\": \"She kept the secret hidden from everyone.\",\n",
        "    \"small\": \"The small kitten was very playful.\",\n",
        "    \"mountain\": \"The mountain was covered in thick forests.\",\n",
        "    \"part\": \"Each part of the puzzle was important.\",\n",
        "    \"everyday\": \"He wore his everyday clothes to the party.\",\n",
        "    \"life\": \"Life in the village was peaceful.\",\n",
        "    \"nestled\": \"The cabin was nestled in the woods.\",\n",
        "    \"towering\": \"The towering trees made the forest dark and cool.\"\n",
        "}\n",
        "\n",
        "# Pronouns and specific words to be excluded\n",
        "exclude_words = set([\n",
        "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
        "    'my', 'your', 'his', 'its', 'our', 'their', 'mine', 'yours', 'hers', 'ours', 'theirs',\n",
        "    'alex', 'mia', 'sam', 'echo', 'ridge', 'guardian', 'of', 'the', 'glen'\n",
        "])\n",
        "\n",
        "def process_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "    word_freq = Counter(words)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    return word_freq, pos_tags\n",
        "\n",
        "def generate_wordcloud(word_freq):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.savefig('wordcloud.png')\n",
        "    return 'wordcloud.png'\n",
        "\n",
        "def translate_and_get_pos(word_freq, pos_tags):\n",
        "    pos_map = {\n",
        "        'NN': 'n.', 'NNS': 'n.', 'VB': 'v.', 'VBD': 'v.', 'VBG': 'v.', 'VBN': 'v.',\n",
        "        'VBP': 'v.', 'VBZ': 'v.', 'JJ': 'adj.', 'JJR': 'adj.', 'JJS': 'adj.', 'RB': 'adv.',\n",
        "        'RBR': 'adv.', 'RBS': 'adv.'\n",
        "    }\n",
        "\n",
        "    word_data = []\n",
        "    for word, freq in word_freq.items():\n",
        "        pos = [pos_tag[1] for pos_tag in pos_tags if pos_tag[0] == word]\n",
        "        if pos and (pos[0] in ['NNP', 'NNPS'] or word in exclude_words):\n",
        "            continue  # Skip proper nouns, pronouns, and specific excluded words\n",
        "        translation = translator.translate(word, dest='ko').text\n",
        "        pos = pos_map.get(pos[0], 'N/A') if pos else 'N/A'\n",
        "        example_sentence = example_sentences.get(word, f\"ex) The word '{word}' in a sentence.\")\n",
        "        synonyms = 'resident, dweller, local people'  # Placeholder for synonyms\n",
        "        word_data.append((word, freq, translation, pos, example_sentence, synonyms))\n",
        "    word_data.sort(key=lambda x: x[1], reverse=True)\n",
        "    return word_data[:50]\n",
        "\n",
        "def main(text):\n",
        "    word_freq, pos_tags = process_text(text)\n",
        "    wordcloud_image = generate_wordcloud(word_freq)\n",
        "    word_data = translate_and_get_pos(word_freq, pos_tags)\n",
        "\n",
        "    word_data_str = \"\\n\".join([f\"{i+1}. {word}: {pos} {translation}, ex) {example_sentence} 동의어: {synonyms}.\" for i, (word, freq, translation, pos, example_sentence, synonyms) in enumerate(word_data)])\n",
        "    return wordcloud_image, word_data_str\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=main,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"image\", \"text\"],\n",
        "    title=\"Language Learning App\",\n",
        "    description=\"Input text to generate a word cloud and a frequency list with Korean meanings, parts of speech, and example sentences.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "nEsK39MZ1RRR",
        "outputId": "1a3d8df8-1d62-4417-e991-94f9bcbe1c3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://aed75661642806ed70.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://aed75661642806ed70.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import stopwords\n",
        "import gradio as gr\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "translator = Translator()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define example sentences and synonyms for the word list\n",
        "word_data_examples = {\n",
        "    \"village\": (\"The village was quiet at night.\", \"hamlet, community\"),\n",
        "    \"adventure\": (\"They went on an exciting adventure in the forest.\", \"expedition, quest\"),\n",
        "    \"map\": (\"We used a map to find the hidden treasure.\", \"chart, atlas\"),\n",
        "    \"cave\": (\"They explored a dark cave in the mountains.\", \"cavern, grotto\"),\n",
        "    \"among\": (\"She found her book among the pile of papers.\", \"amidst, between\"),\n",
        "    \"mountains\": (\"The mountains were covered with snow in winter.\", \"peaks, ranges\"),\n",
        "    \"children\": (\"The children played games in the park.\", \"kids, youngsters\"),\n",
        "    \"known\": (\"He was known for his kindness and bravery.\", \"recognized, famous\"),\n",
        "    \"hidden\": (\"They found a hidden door behind the bookshelf.\", \"concealed, secret\"),\n",
        "    \"local\": (\"The local market was full of fresh produce.\", \"regional, native\"),\n",
        "    \"discovery\": (\"The discovery of the old map excited everyone.\", \"finding, revelation\"),\n",
        "    \"eagle\": (\"An eagle soared high above the valley.\", \"raptor, bird of prey\"),\n",
        "    \"villagers\": (\"The villagers gathered in the square for the festival.\", \"residents, townsfolk\"),\n",
        "    \"legend\": (\"The legend of the lost city intrigued the adventurers.\", \"myth, lore\"),\n",
        "    \"tales\": (\"Grandma told us tales of her childhood.\", \"stories, narratives\"),\n",
        "    \"daring\": (\"His daring escape from the cave was legendary.\", \"bold, audacious\"),\n",
        "    \"spirit\": (\"The spirit of adventure was alive in their hearts.\", \"soul, essence\"),\n",
        "    \"exploring\": (\"They spent the summer exploring the forest.\", \"investigating, discovering\"),\n",
        "    \"old\": (\"The old castle was full of secrets.\", \"ancient, aged\"),\n",
        "    \"lost\": (\"He felt lost without his best friend.\", \"missing, misplaced\"),\n",
        "    \"ancient\": (\"They discovered ancient artifacts in the desert.\", \"archaic, antique\"),\n",
        "    \"inside\": (\"Inside the box was a beautiful necklace.\", \"within, interior\"),\n",
        "    \"treasure\": (\"They dreamed of finding hidden treasure.\", \"riches, valuables\"),\n",
        "    \"whispering\": (\"The trees were whispering secrets in the wind.\", \"murmuring, softly speaking\"),\n",
        "    \"hollow\": (\"They found a hollow tree to hide in during the storm.\", \"cavity, void\"),\n",
        "    \"decided\": (\"She decided to take the long way home.\", \"determined, resolved\"),\n",
        "    \"journey\": (\"Their journey took them across the country.\", \"trip, voyage\"),\n",
        "    \"together\": (\"They worked together to solve the mystery.\", \"jointly, collectively\"),\n",
        "    \"way\": (\"She found a new way to solve the puzzle.\", \"method, manner\"),\n",
        "    \"reached\": (\"They finally reached the top of the hill.\", \"arrived, attained\"),\n",
        "    \"chest\": (\"The chest was filled with gold coins.\", \"trunk, box\"),\n",
        "    \"boulder\": (\"A large boulder blocked the path.\", \"rock, stone\"),\n",
        "    \"artifacts\": (\"The museum displayed artifacts from ancient Egypt.\", \"relics, antiquities\"),\n",
        "    \"legends\": (\"The legends spoke of a hidden kingdom.\", \"myths, sagas\"),\n",
        "    \"explore\": (\"They wanted to explore the old mansion.\", \"investigate, examine\"),\n",
        "    \"secret\": (\"She kept the secret hidden from everyone.\", \"confidential, hidden\"),\n",
        "    \"small\": (\"The small kitten was very playful.\", \"tiny, little\"),\n",
        "    \"mountain\": (\"The mountain was covered in thick forests.\", \"peak, hill\"),\n",
        "    \"part\": (\"Each part of the puzzle was important.\", \"piece, segment\"),\n",
        "    \"everyday\": (\"He wore his everyday clothes to the party.\", \"daily, routine\"),\n",
        "    \"life\": (\"Life in the village was peaceful.\", \"existence, being\"),\n",
        "    \"nestled\": (\"The cabin was nestled in the woods.\", \"tucked, situated\"),\n",
        "    \"towering\": (\"The towering trees made the forest dark and cool.\", \"lofty, soaring\"),\n",
        "    \"peaks\": (\"The mountain peaks were covered in snow.\", \"summits, crests\"),\n",
        "    \"said\": (\"He said he would be back soon.\", \"stated, remarked\"),\n",
        "    \"protected\": (\"The ancient ruins were protected by law.\", \"guarded, sheltered\"),\n",
        "    \"massive\": (\"The massive ship docked at the port.\", \"enormous, huge\"),\n",
        "    \"supposedly\": (\"The treasure was supposedly buried under the tree.\", \"allegedly, reportedly\"),\n",
        "    \"watched\": (\"They watched the movie together.\", \"observed, viewed\"),\n",
        "    \"perch\": (\"The bird found a perch on the windowsill.\", \"roost, rest\")\n",
        "}\n",
        "\n",
        "# Words to be excluded from both the word cloud and the word list\n",
        "exclude_words = set([\n",
        "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',\n",
        "    'my', 'your', 'his', 'its', 'our', 'their', 'mine', 'yours', 'hers', 'ours', 'theirs',\n",
        "    'alex', 'mia', 'sam', 'echo', 'ridge', 'guardian', 'of', 'the', 'glen'\n",
        "])\n",
        "\n",
        "def process_text(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words and word.lower() not in exclude_words]\n",
        "    word_freq = Counter(words)\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    return word_freq, pos_tags\n",
        "\n",
        "def generate_wordcloud(word_freq):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.savefig('wordcloud.png')\n",
        "    return 'wordcloud.png'\n",
        "\n",
        "def translate_and_get_pos(word_freq, pos_tags):\n",
        "    pos_map = {\n",
        "        'NN': 'n.', 'NNS': 'n.', 'VB': 'v.', 'VBD': 'v.', 'VBG': 'v.', 'VBN': 'v.',\n",
        "        'VBP': 'v.', 'VBZ': 'v.', 'JJ': 'adj.', 'JJR': 'adj.', 'JJS': 'adj.', 'RB': 'adv.',\n",
        "        'RBR': 'adv.', 'RBS': 'adv.'\n",
        "    }\n",
        "\n",
        "    word_data = []\n",
        "    for word, freq in word_freq.items():\n",
        "        pos = [pos_tag[1] for pos_tag in pos_tags if pos_tag[0] == word]\n",
        "        if pos and (pos[0] in ['NNP', 'NNPS'] or word in exclude_words):\n",
        "            continue  # Skip proper nouns, pronouns, and specific excluded words\n",
        "        translation = translator.translate(word, dest='ko').text\n",
        "        pos = pos_map.get(pos[0], 'N/A') if pos else 'N/A'\n",
        "        example_sentence, synonyms = word_data_examples.get(word, (f\"ex) The word '{word}' in a sentence.\", \"\"))\n",
        "        word_data.append((word, freq, translation, pos, example_sentence, synonyms))\n",
        "    word_data.sort(key=lambda x: x[1], reverse=True)\n",
        "    return word_data[:50]\n",
        "\n",
        "def main(text):\n",
        "    word_freq, pos_tags = process_text(text)\n",
        "    wordcloud_image = generate_wordcloud(word_freq)\n",
        "    word_data = translate_and_get_pos(word_freq, pos_tags)\n",
        "\n",
        "    word_data_str = \"\\n\".join([f\"{i+1}. {word}: {pos} {translation}, ex) {example_sentence} 동의어: {synonyms}.\" for i, (word, freq, translation, pos, example_sentence, synonyms) in enumerate(word_data)])\n",
        "    return wordcloud_image, word_data_str\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=main,\n",
        "    inputs=\"text\",\n",
        "    outputs=[\"image\", \"text\"],\n",
        "    title=\"Language Learning App\",\n",
        "    description=\"Input text to generate a word cloud and a frequency list with Korean meanings, parts of speech, and example sentences.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "UD6d-vio2zFl",
        "outputId": "6751397f-5508-4567-9087-6cebf8bd7973"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://b9972a7b0d0bd0542c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b9972a7b0d0bd0542c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/spaces/teatwots/wordcloud\n",
        "!cp app.py wordcloud/\n",
        "!cp requirements.txt wordcloud/\n",
        "!cd wordcloud && git add app.py requirements.txt && git commit -m \"Update app script and requirements\" && git push\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9vtjW7m6DWe",
        "outputId": "f0346359-5280-4d0f-e198-92869a30d4b7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'wordcloud' already exists and is not an empty directory.\n",
            "cp: cannot stat 'app.py': No such file or directory\n",
            "cp: cannot stat 'requirements.txt': No such file or directory\n",
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@fc99def946a7.(none)')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/spaces/teatwots/wordcloud\n",
        "!cp app.py your-space-name/\n",
        "!cp styles.css your-space-name/\n",
        "!cd your-space-name && git add app.py styles.css && git commit -m \"Update app script and add custom styles\" && git push\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex2gTxak8-93",
        "outputId": "01b73c2d-a5b5-44d2-ce88-4453b02742d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'wordcloud' already exists and is not an empty directory.\n",
            "cp: cannot stat 'app.py': No such file or directory\n",
            "cp: cannot stat 'styles.css': No such file or directory\n",
            "/bin/bash: line 1: cd: your-space-name: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/spaces/teatwots/wordcloud\n",
        "!cp app.py your-space-name/\n",
        "!cp styles.css your-space-name/\n",
        "!cd your-space-name && git add app.py styles.css && git commit -m \"Update app script and add custom styles\" && git push\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1Je83379-iG",
        "outputId": "3ea9e4a8-16dd-4de4-b1c5-72e689d78dca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'wordcloud' already exists and is not an empty directory.\n",
            "cp: cannot stat 'app.py': No such file or directory\n",
            "cp: cannot stat 'styles.css': No such file or directory\n",
            "/bin/bash: line 1: cd: your-space-name: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git-lfs\n",
        "\n",
        "!git clone https://huggingface.co/spaces/teatwots/wordcloud\n",
        "!cp app.py wordcloud/\n",
        "!cp styles.css wordcloud/\n",
        "!cd wordcloud && git add app.py styles.css && git commit -m \"Update app script and add custom styles\" && git push\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoUromaI-pQc",
        "outputId": "9551fb49-fff7-4809-b64d-34d95159fc81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git-lfs\n",
            "  Downloading git_lfs-1.6-py2.py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: git-lfs\n",
            "Successfully installed git-lfs-1.6\n",
            "fatal: destination path 'wordcloud' already exists and is not an empty directory.\n",
            "cp: cannot stat 'app.py': No such file or directory\n",
            "cp: cannot stat 'styles.css': No such file or directory\n",
            "fatal: pathspec 'styles.css' did not match any files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git-lfs\n",
        "\n",
        "!git clone https://huggingface.co/spaces/teatwots/wordcloud\n",
        "!cp app.py wordcloud/\n",
        "!cd wordcloud && git add app.py && git commit -m \"Update app script with custom CSS\" && git push\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_WG2pxB_-FU",
        "outputId": "58387878-def4-4444-e4c1-b18c51314176"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: git-lfs in /usr/local/lib/python3.10/dist-packages (1.6)\n",
            "fatal: destination path 'wordcloud' already exists and is not an empty directory.\n",
            "cp: cannot stat 'app.py': No such file or directory\n",
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@fc99def946a7.(none)')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git-lfs\n",
        "\n",
        "!git clone https://huggingface.co/spaces/teatwots/wordcloud\n",
        "!cp app.py wordcloud/\n",
        "!cd wordcloud && git add app.py && git commit -m \"Update app script with custom CSS\" && git push\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSmAM9OzAvX9",
        "outputId": "d985dee5-36ee-4933-ff0f-711f8cae9384"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: git-lfs in /usr/local/lib/python3.10/dist-packages (1.6)\n",
            "fatal: destination path 'wordcloud' already exists and is not an empty directory.\n",
            "cp: cannot stat 'app.py': No such file or directory\n",
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@fc99def946a7.(none)')\n"
          ]
        }
      ]
    }
  ]
}